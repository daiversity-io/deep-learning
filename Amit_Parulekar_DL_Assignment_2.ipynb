{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mtxHdedBFm5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete the functions and the other sections below to build a complete model of a simple neural network (equivalent to Logistic Regression). \n",
    "The incomplete parts are marked and you need to fill them up to finish this assignment\n",
    "\n",
    "DATA: handwritten digits classified into two classes Even (0) and Odd (1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "##Loading the data\n",
    "digits = datasets.load_digits()\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "\n",
    "train_set_x_orig = [] \n",
    "train_set_y = []\n",
    "test_set_x_orig = [] \n",
    "test_set_y = []\n",
    "classes = ['even','odd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DSx6t07CBKng",
    "outputId": "805678d1-0693-46a2-f3b3-e3f253b69a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1597, 8, 8) (1, 1597) (200, 8, 8) (1, 200)\n"
     ]
    }
   ],
   "source": [
    "data_size = len(images_and_labels)\n",
    "#Setting the testset size\n",
    "test_set_size = 200\n",
    "\n",
    "\n",
    "##Splitting the data into training and test sets and assigning the labels: Even (0), Odd (1)\n",
    "\n",
    "for i in range(len(images_and_labels))[:-test_set_size]:\n",
    "    train_set_x_orig.append(images_and_labels[i][0])\n",
    "    if images_and_labels[i][1] %2 == 0: ##if even put 0 as the label\n",
    "        train_set_y.append(0)\n",
    "    else: ##if odd put 1 as the label\n",
    "        train_set_y.append(1)\n",
    "        \n",
    "for i in range(len(images_and_labels))[-test_set_size:]:\n",
    "    test_set_x_orig.append(images_and_labels[i][0])\n",
    "    if images_and_labels[i][1] %2 == 0: ##if even put 0 as the label\n",
    "        test_set_y.append(0)\n",
    "    else: ##if odd put 1 as the label\n",
    "        test_set_y.append(1)\n",
    "\n",
    "train_set_x_orig = np.array(train_set_x_orig)\n",
    "train_set_y = np.array(train_set_y).reshape(1,data_size-test_set_size)\n",
    "test_set_x_orig = np.array(test_set_x_orig)\n",
    "test_set_y = np.array(test_set_y).reshape(1,test_set_size)\n",
    "\n",
    "print(train_set_x_orig.shape, train_set_y.shape, test_set_x_orig.shape, test_set_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "LbHcLuSaBY0p",
    "outputId": "aa71d50b-db78-40a3-82c2-8420bf045ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'odd' number.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display an example from the data\n",
    "index = 1\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])] +  \"' number.\")\n",
    "\n",
    "m_train = train_set_y.shape[1]\n",
    "m_test = test_set_y.shape[1]\n",
    "num_px = train_set_x_orig.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "G_GTLCt_Bizi",
    "outputId": "57cbab1a-a784-4fff-fca2-c4e61a2662ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 1597\n",
      "Number of testing examples: m_test = 200\n",
      "Height/Width of each image: num_px = 8\n",
      "Each image is of size: (8, 8)\n",
      "train_set_x shape: (1597, 8, 8)\n",
      "train_set_y shape: (1, 1597)\n",
      "test_set_x shape: (200, 8, 8)\n",
      "test_set_y shape: (1, 200)\n"
     ]
    }
   ],
   "source": [
    "##Description of the data\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \")\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "9vSS9w2tBo_F",
    "outputId": "ea0541b6-651d-4ead-d2d7-763f62bb3f92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (64, 1597)\n",
      "train_set_y shape: (1, 1597)\n",
      "test_set_x_flatten shape: (64, 200)\n",
      "test_set_y shape: (1, 200)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples from 2D matrix to a 1D vector. The input vector of a neural network is always 1D\n",
    "train_set_x_flatten = train_set_x_orig.reshape(m_train,(num_px*num_px)).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(m_test,(num_px*num_px)).T\n",
    "\n",
    "# Print the description after reshaping\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dt6Pd70_BtHY"
   },
   "outputs": [],
   "source": [
    "# Standardize the pixel value\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nWnN_jS6ha-T",
    "outputId": "fcdd1a5d-28f2-4936-d209-0b397b23e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.664420\n",
      "Cost after iteration 200: 0.638784\n",
      "Cost after iteration 300: 0.615838\n",
      "Cost after iteration 400: 0.595245\n",
      "Cost after iteration 500: 0.576706\n",
      "Cost after iteration 600: 0.559960\n",
      "Cost after iteration 700: 0.544782\n",
      "Cost after iteration 800: 0.530975\n",
      "Cost after iteration 900: 0.518371\n",
      "Cost after iteration 1000: 0.506826\n",
      "Cost after iteration 1100: 0.496216\n",
      "Cost after iteration 1200: 0.486433\n",
      "Cost after iteration 1300: 0.477384\n",
      "Cost after iteration 1400: 0.468990\n",
      "Cost after iteration 1500: 0.461181\n",
      "Cost after iteration 1600: 0.453896\n",
      "Cost after iteration 1700: 0.447083\n",
      "Cost after iteration 1800: 0.440697\n",
      "Cost after iteration 1900: 0.434696\n",
      "Cost after iteration 2000: 0.429045\n",
      "Cost after iteration 2100: 0.423713\n",
      "Cost after iteration 2200: 0.418673\n",
      "Cost after iteration 2300: 0.413898\n",
      "Cost after iteration 2400: 0.409368\n",
      "Cost after iteration 2500: 0.405062\n",
      "Cost after iteration 2600: 0.400964\n",
      "Cost after iteration 2700: 0.397058\n",
      "Cost after iteration 2800: 0.393328\n",
      "Cost after iteration 2900: 0.389764\n",
      "Cost after iteration 3000: 0.386352\n",
      "Cost after iteration 3100: 0.383082\n",
      "Cost after iteration 3200: 0.379945\n",
      "Cost after iteration 3300: 0.376933\n",
      "Cost after iteration 3400: 0.374037\n",
      "Cost after iteration 3500: 0.371249\n",
      "Cost after iteration 3600: 0.368564\n",
      "Cost after iteration 3700: 0.365976\n",
      "Cost after iteration 3800: 0.363478\n",
      "Cost after iteration 3900: 0.361065\n",
      "Cost after iteration 4000: 0.358733\n",
      "Cost after iteration 4100: 0.356477\n",
      "Cost after iteration 4200: 0.354294\n",
      "Cost after iteration 4300: 0.352178\n",
      "Cost after iteration 4400: 0.350128\n",
      "Cost after iteration 4500: 0.348139\n",
      "Cost after iteration 4600: 0.346209\n",
      "Cost after iteration 4700: 0.344335\n",
      "Cost after iteration 4800: 0.342513\n",
      "Cost after iteration 4900: 0.340743\n",
      "Cost after iteration 5000: 0.339020\n",
      "Cost after iteration 5100: 0.337344\n",
      "Cost after iteration 5200: 0.335711\n",
      "Cost after iteration 5300: 0.334121\n",
      "Cost after iteration 5400: 0.332571\n",
      "Cost after iteration 5500: 0.331060\n",
      "Cost after iteration 5600: 0.329586\n",
      "Cost after iteration 5700: 0.328147\n",
      "Cost after iteration 5800: 0.326742\n",
      "Cost after iteration 5900: 0.325371\n",
      "Cost after iteration 6000: 0.324030\n",
      "Cost after iteration 6100: 0.322721\n",
      "Cost after iteration 6200: 0.321440\n",
      "Cost after iteration 6300: 0.320187\n",
      "Cost after iteration 6400: 0.318962\n",
      "Cost after iteration 6500: 0.317762\n",
      "Cost after iteration 6600: 0.316588\n",
      "Cost after iteration 6700: 0.315438\n",
      "Cost after iteration 6800: 0.314311\n",
      "Cost after iteration 6900: 0.313207\n",
      "Cost after iteration 7000: 0.312125\n",
      "Cost after iteration 7100: 0.311065\n",
      "Cost after iteration 7200: 0.310024\n",
      "Cost after iteration 7300: 0.309004\n",
      "Cost after iteration 7400: 0.308003\n",
      "Cost after iteration 7500: 0.307020\n",
      "Cost after iteration 7600: 0.306056\n",
      "Cost after iteration 7700: 0.305109\n",
      "Cost after iteration 7800: 0.304179\n",
      "Cost after iteration 7900: 0.303265\n",
      "Cost after iteration 8000: 0.302367\n",
      "Cost after iteration 8100: 0.301485\n",
      "Cost after iteration 8200: 0.300618\n",
      "Cost after iteration 8300: 0.299766\n",
      "Cost after iteration 8400: 0.298928\n",
      "Cost after iteration 8500: 0.298103\n",
      "Cost after iteration 8600: 0.297292\n",
      "Cost after iteration 8700: 0.296495\n",
      "Cost after iteration 8800: 0.295709\n",
      "Cost after iteration 8900: 0.294937\n",
      "Cost after iteration 9000: 0.294176\n",
      "Cost after iteration 9100: 0.293427\n",
      "Cost after iteration 9200: 0.292690\n",
      "Cost after iteration 9300: 0.291964\n",
      "Cost after iteration 9400: 0.291249\n",
      "Cost after iteration 9500: 0.290544\n",
      "Cost after iteration 9600: 0.289850\n",
      "Cost after iteration 9700: 0.289166\n",
      "Cost after iteration 9800: 0.288491\n",
      "Cost after iteration 9900: 0.287827\n",
      "Cost after iteration 10000: 0.287171\n",
      "Cost after iteration 10100: 0.286525\n",
      "Cost after iteration 10200: 0.285888\n",
      "Cost after iteration 10300: 0.285260\n",
      "Cost after iteration 10400: 0.284640\n",
      "Cost after iteration 10500: 0.284028\n",
      "Cost after iteration 10600: 0.283425\n",
      "Cost after iteration 10700: 0.282830\n",
      "Cost after iteration 10800: 0.282243\n",
      "Cost after iteration 10900: 0.281663\n",
      "Cost after iteration 11000: 0.281091\n",
      "Cost after iteration 11100: 0.280526\n",
      "Cost after iteration 11200: 0.279968\n",
      "Cost after iteration 11300: 0.279417\n",
      "Cost after iteration 11400: 0.278873\n",
      "Cost after iteration 11500: 0.278336\n",
      "Cost after iteration 11600: 0.277806\n",
      "Cost after iteration 11700: 0.277282\n",
      "Cost after iteration 11800: 0.276764\n",
      "Cost after iteration 11900: 0.276252\n",
      "Cost after iteration 12000: 0.275747\n",
      "Cost after iteration 12100: 0.275247\n",
      "Cost after iteration 12200: 0.274754\n",
      "Cost after iteration 12300: 0.274266\n",
      "Cost after iteration 12400: 0.273784\n",
      "Cost after iteration 12500: 0.273307\n",
      "Cost after iteration 12600: 0.272836\n",
      "Cost after iteration 12700: 0.272370\n",
      "Cost after iteration 12800: 0.271909\n",
      "Cost after iteration 12900: 0.271453\n",
      "Cost after iteration 13000: 0.271002\n",
      "Cost after iteration 13100: 0.270557\n",
      "Cost after iteration 13200: 0.270116\n",
      "Cost after iteration 13300: 0.269680\n",
      "Cost after iteration 13400: 0.269248\n",
      "Cost after iteration 13500: 0.268821\n",
      "Cost after iteration 13600: 0.268399\n",
      "Cost after iteration 13700: 0.267981\n",
      "Cost after iteration 13800: 0.267567\n",
      "Cost after iteration 13900: 0.267158\n",
      "Cost after iteration 14000: 0.266753\n",
      "Cost after iteration 14100: 0.266352\n",
      "Cost after iteration 14200: 0.265955\n",
      "Cost after iteration 14300: 0.265562\n",
      "Cost after iteration 14400: 0.265173\n",
      "Cost after iteration 14500: 0.264788\n",
      "Cost after iteration 14600: 0.264407\n",
      "Cost after iteration 14700: 0.264029\n",
      "Cost after iteration 14800: 0.263655\n",
      "Cost after iteration 14900: 0.263285\n",
      "Cost after iteration 15000: 0.262918\n",
      "Cost after iteration 15100: 0.262555\n",
      "Cost after iteration 15200: 0.262195\n",
      "Cost after iteration 15300: 0.261839\n",
      "Cost after iteration 15400: 0.261486\n",
      "Cost after iteration 15500: 0.261136\n",
      "Cost after iteration 15600: 0.260790\n",
      "Cost after iteration 15700: 0.260446\n",
      "Cost after iteration 15800: 0.260106\n",
      "Cost after iteration 15900: 0.259769\n",
      "Cost after iteration 16000: 0.259435\n",
      "Cost after iteration 16100: 0.259104\n",
      "Cost after iteration 16200: 0.258776\n",
      "Cost after iteration 16300: 0.258451\n",
      "Cost after iteration 16400: 0.258128\n",
      "Cost after iteration 16500: 0.257809\n",
      "Cost after iteration 16600: 0.257492\n",
      "Cost after iteration 16700: 0.257178\n",
      "Cost after iteration 16800: 0.256867\n",
      "Cost after iteration 16900: 0.256558\n",
      "Cost after iteration 17000: 0.256252\n",
      "Cost after iteration 17100: 0.255949\n",
      "Cost after iteration 17200: 0.255648\n",
      "Cost after iteration 17300: 0.255350\n",
      "Cost after iteration 17400: 0.255054\n",
      "Cost after iteration 17500: 0.254761\n",
      "Cost after iteration 17600: 0.254470\n",
      "Cost after iteration 17700: 0.254182\n",
      "Cost after iteration 17800: 0.253895\n",
      "Cost after iteration 17900: 0.253612\n",
      "Cost after iteration 18000: 0.253330\n",
      "Cost after iteration 18100: 0.253051\n",
      "Cost after iteration 18200: 0.252774\n",
      "Cost after iteration 18300: 0.252499\n",
      "Cost after iteration 18400: 0.252226\n",
      "Cost after iteration 18500: 0.251955\n",
      "Cost after iteration 18600: 0.251687\n",
      "Cost after iteration 18700: 0.251421\n",
      "Cost after iteration 18800: 0.251156\n",
      "Cost after iteration 18900: 0.250894\n",
      "Cost after iteration 19000: 0.250634\n",
      "Cost after iteration 19100: 0.250376\n",
      "Cost after iteration 19200: 0.250119\n",
      "Cost after iteration 19300: 0.249865\n",
      "Cost after iteration 19400: 0.249612\n",
      "Cost after iteration 19500: 0.249362\n",
      "Cost after iteration 19600: 0.249113\n",
      "Cost after iteration 19700: 0.248866\n",
      "Cost after iteration 19800: 0.248621\n",
      "Cost after iteration 19900: 0.248378\n",
      "train accuracy: 90.98309329993738 %\n",
      "test accuracy: 90.0 %\n"
     ]
    }
   ],
   "source": [
    "###COMPLETE THE FUNCTION\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    ## Put your code here\n",
    "    s = 1 / (1 + np.exp(-z))    \n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "###COMPLETE THE FUNCTION\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Put your code here\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    \n",
    "    ## veryifying the shape of the w vector\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "\n",
    "###COMPLETE THE FUNCTION\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if even and 1 if odd) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = sigmoid(np.dot(w.T, X) + b) # put the activation computation code here\n",
    "    cost = (- 1 / m) * np.sum((Y * np.log(A)) + (1 - Y)*np.log(1-A)) # put the cost computation code here\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRADIENTS)\n",
    "    \n",
    "    dw = 1 / m * (np.dot(X, (A - Y).T)) # put the code to compute dw\n",
    "    db =  1 / m * (np.sum(A - Y)) # put the code to compute db\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###COMPLETE THE FUNCTION\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if even, 1 if odd), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "                 \n",
    "        grads, cost = propagate(w, b, X, Y) # call the function that does this part\n",
    "        \n",
    "        \n",
    "        \n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        w = w - (dw * learning_rate) # update w using dw and learning rate\n",
    "        b = b - (db * learning_rate) # update b using db and learning rate\n",
    "        \n",
    "        # Recordint the cost after every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###COMPLETE THE FUNCTION\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1) in this case (8 * 8, 1)\n",
    "    b -- bias, a scalar \n",
    "    X -- data of size (num_px * num_px, number of examples) in this case (8 * 8, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    ## put the code to predict from input data [prediction refers to computing the 'a' variable or sigmoid of w.T*x+b, \n",
    "    ## where w,b is the learned parameters and x is the input vector of the image you are trying to predict]\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i], i.e. if probability >=0.5 output 1, else 0\n",
    "        if A[0, i] > 0.5: ##complete this line\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0 \n",
    "        \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "\n",
    "\n",
    "###COMPLETE THE FUNCTION\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### COMPLETE THE CODE BELOW ###\n",
    "    \n",
    "    # initialize parameters with zeros \n",
    "    w, b = w, b = initialize_with_zeros(X_train.shape[0]) # call the correct function from above\n",
    "\n",
    "    # Gradient descent \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # call the correct function from above\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\" returned by the function above\n",
    "    w = parameters[\"w\"] # complete this line\n",
    "    b = parameters[\"b\"] # complete this line\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(w, b, X_test) # call the correct function from above\n",
    "    Y_prediction_train = predict(w, b, X_train) # call the correct function from above\n",
    "\n",
    "    \n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n",
    "\n",
    "## Calling the function to train the model. FEEL FREE TO MODIFY THE HYPERPARAMETERS (num_iterations AND learning_rate) to experiment with the accuracy\n",
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 20000, learning_rate = 1, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7Ikerjb_HIY2",
    "outputId": "5cd5d5cb-2e9e-4a09-ba97-5ca69f757c29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 9\n",
    "# test_set_y[0,index]\n",
    "d['Y_prediction_test'][0,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "aVO-zGujCOEf",
    "outputId": "88d1f110-b941-45f8-c4ac-18a23899307b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1, you predicted that it is a \"odd\" number.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Using the model you built to predict the digit of an image\n",
    "index = 9 ## put a number between 0 and test_set_size to see the prediction of that image\n",
    "plt.imshow(test_set_x[:,index].reshape((num_px, num_px)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(d['Y_prediction_test'][0,index])] +  \"\\\" number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "KMzCaU0wLKxQ",
    "outputId": "cd3e65d1-c71f-402b-dfaf-c506229f530b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## View how the cost varied with iterations\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Amit_Parulekar_DL Assignment 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
